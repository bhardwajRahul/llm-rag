{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b36494-274b-43f4-a307-c1b9e5253b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from IPython.display import display, Image\n",
    "from rich import print as rprint\n",
    "from rich.markdown import Markdown\n",
    "from rich.pretty import Pretty\n",
    "from rich.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664bd98e-5d0b-48ef-857d-44d03b90fc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv('.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4c6966-8773-4f76-b912-e7a62752bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG From Scratch: Part 16 (Generation - Self-RAG)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b60d17-f30f-44e5-b6b9-b7ecdbf8ceeb",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc86a5-f35d-4516-bbbe-9736d5435a7c",
   "metadata": {},
   "source": [
    "![](images/generation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8889a307-fa3f-4d38-9127-d41e4686ae47",
   "metadata": {},
   "source": [
    "# Self-RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30920b5-6d77-4e47-befb-4efff989cb9e",
   "metadata": {},
   "source": [
    "![](images/16-self-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d826c-ad1d-4d65-8341-59dbac76053c",
   "metadata": {},
   "source": [
    "![](images/16-self-rag-implementation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd601f-39bd-454b-a21a-990a039811b8",
   "metadata": {},
   "source": [
    "## Configure components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24f2640-f366-4df2-afd9-e5a0fc5bc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af2d609f-8904-4c41-a029-817d1871a233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 8, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_38647f5e19', 'id': 'chatcmpl-BQDpuUuq3GxM93181kPv9KAezyixR', 'finish_reason': 'stop', 'logprobs': None}, id='run-357964f7-f3aa-4974-9faa-d340e177cca9-0', usage_metadata={'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=1\n",
    ")\n",
    "llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "936c53fc-e31d-4948-8db0-c64072f2acf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "len(embeddings.embed_query(\"Hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310364c-7f1b-4660-86f6-757f0cb81a42",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efdb52b7-6d03-402a-a9e8-349b6afb8b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33acb22b-4015-4cf8-b1ef-8341e607a598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=articles,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef525ba0-bfd0-484f-82ec-4aa4ed213915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a2d79a-83d1-4106-9b1f-9f9699cfb14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Prompt Engineering\n",
      "    \n",
      "Date: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
      "This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\n",
      "[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b320811-4089-4590-a499-24d701fb9b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Adversarial Attacks on LLMs\n",
      "    \n",
      "Date: October 25, 2023  |  Estimated Reading Time: 33 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\n",
      "A large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.\n",
      "There is also a branch of work on attackin\n"
     ]
    }
   ],
   "source": [
    "print(docs[2].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2efc2ed-bde8-4152-b867-a685b1543ddc",
   "metadata": {},
   "source": [
    "## Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "999dfbf7-07e8-4b7b-aeab-e3ad9f9b2347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b89d7902-b90f-4918-a20f-c90315d6d708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00516264-bf6a-4972-b997-053aaf13c2a1",
   "metadata": {},
   "source": [
    "## Store documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2676ca99-2159-4026-9015-415fd3188e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5419c0e5-b712-41ae-82b9-c4a0a1a84fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 180)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = InMemoryVectorStore(embeddings)\n",
    "doc_ids = vectorstore.add_documents(documents=splits)\n",
    "len(doc_ids), len(vectorstore.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feb37e81-2999-480c-b504-7025ddf7a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809144c-6bf0-4512-9fa3-520f981efd00",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31da3a96-cbb6-4749-b619-51b9cfed3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import chain\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.types import Command\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bf3dc00-ccde-433d-9327-e7f52e03f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are grader assistant assessing the need to retrieve additional documents to answer the user's question.\n",
      "If you are sure that all the necessary data is available, then you do not need to retrieve additional documents.\n",
      "Give a binary score to indicate whether retrieval is required.\n",
      "\n",
      "User question:\n",
      "{question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval_prompt_template = \"\"\"You are grader assistant assessing the need to retrieve additional documents to answer the user's question.\n",
    "If you are sure that all the necessary data is available, then you do not need to retrieve additional documents.\n",
    "Give a binary score to indicate whether retrieval is required.\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\"\"\"\n",
    "print(retrieval_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e3ce2f0-8fa2-413f-b065-a9fe06132508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question based on this context:\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_prompt_template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "print(rag_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fed2035-c9d6-4759-8e07-e43f8e8a273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question:\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_prompt_template = \"\"\"Answer the following question:\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "print(answer_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cadb6159-49ac-4c41-bd78-c03a75fdecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have an answer to the question.\n"
     ]
    }
   ],
   "source": [
    "no_answer_prompt = \"I don't have an answer to the question.\"\n",
    "print(no_answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcffecf4-d9f4-437b-a014-82160a2b30fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a grader assessing relevance of a retrieved document to a user question.\n",
      "It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
      "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
      "Give a binary score to indicate whether the document is relevant to the question.\n",
      "\n",
      "Retrieved document:\n",
      "{document}\n",
      "\n",
      "User question:\n",
      "{question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevance_grading_prompt_template = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.\n",
    "Give a binary score to indicate whether the document is relevant to the question.\n",
    "\n",
    "Retrieved document:\n",
    "{document}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "print(relevance_grading_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c94e2d5d-71e5-452e-bf4b-33c992e2b6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a grader assessing whether an LLM answer is grounded in / supported by a set of retrieved facts.\n",
      "Give a binary score whether the answer is grounded in / supported by the set of facts.\n",
      "\n",
      "Set of facts:\n",
      "{context}\n",
      "\n",
      "LLM answer:\n",
      "{answer}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hallucinations_grading_prompt_template = \"\"\"You are a grader assessing whether an LLM answer is grounded in / supported by a set of retrieved facts.\n",
    "Give a binary score whether the answer is grounded in / supported by the set of facts.\n",
    "\n",
    "Set of facts:\n",
    "{context}\n",
    "\n",
    "LLM answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "print(hallucinations_grading_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "356f9fdf-b84f-4f9a-919a-f8bee7c65ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a grader assessing whether an answer addresses / resolves a question. \n",
      "Give a binary score whether the answer resolves the question.\n",
      "\n",
      "User question:\n",
      "{question}\n",
      "\n",
      "LLM answer:\n",
      "{answer}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_grading_prompt_template = \"\"\"You are a grader assessing whether an answer addresses / resolves a question. \n",
    "Give a binary score whether the answer resolves the question.\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "LLM answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "print(answer_grading_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fc800a2-62f1-4004-9dfb-b5fc427247e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You a question re-writer that converts an input question to a better version that is optimized for web search. \n",
      "Look at the input and try to reason about the underlying semantic intent / meaning.\n",
      "\n",
      "Here is the initial question:\n",
      "{question}\n",
      "\n",
      "Formulate an improved question.\n"
     ]
    }
   ],
   "source": [
    "query_rewriting_prompt_template = \"\"\"You a question re-writer that converts an input question to a better version that is optimized for web search. \n",
    "Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "\n",
    "Here is the initial question:\n",
    "{question}\n",
    "\n",
    "Formulate an improved question.\"\"\"\n",
    "\n",
    "print(query_rewriting_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ebb4e4c-a3ad-474c-9d85-cd2deb33a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalGrade(BaseModel):\n",
    "    \"\"\"Check if retrieval of additional documents is required.\"\"\"\n",
    "    chain_of_thought: str = Field(\n",
    "        ..., description=\"Step by step reasoning to check if retrieval of additional documents is required\"\n",
    "    )\n",
    "    is_required: bool = Field(\n",
    "        description=\"Retrieval of additional documents is required\"\n",
    "    )\n",
    "\n",
    "retrieval_grader_llm = llm.with_structured_output(RetrievalGrade, method=\"function_calling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f97fba9c-f104-412f-8b87-897bce3dc0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevanceGrade(BaseModel):\n",
    "    \"\"\"Relevance check on retrieved document.\"\"\"\n",
    "    chain_of_thought: str = Field(\n",
    "        ..., description=\"Step by step reasoning to check if the document is relevant to the question\"\n",
    "    )\n",
    "    is_relevant: bool = Field(\n",
    "        description=\"Document is relevant to the question\"\n",
    "    )\n",
    "\n",
    "relevance_grader_llm = llm.with_structured_output(RelevanceGrade, method=\"function_calling\")\n",
    "\n",
    "@chain\n",
    "def grade_document_relevance(document, question):\n",
    "    relevance_grading_prompt = relevance_grading_prompt_template.format(document=document, question=question)\n",
    "    response = relevance_grader_llm.invoke([HumanMessage(content=relevance_grading_prompt)])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a40ce122-812d-4294-b7c3-314796e01a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HallucationsGrade(BaseModel):\n",
    "    \"\"\"Hallucination check in generated answer.\"\"\"\n",
    "    chain_of_thought: str = Field(\n",
    "        ..., description=\"Step by step reasoning to check if the answer is grounded in the facts\"\n",
    "    )\n",
    "    is_grounded: bool = Field(\n",
    "        description=\"Answer is grounded in the facts\"\n",
    "    )\n",
    "\n",
    "hallucations_grader_llm = llm.with_structured_output(HallucationsGrade, method=\"function_calling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "880b11b4-9314-41bf-bda6-7e118a877348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGrade(BaseModel):\n",
    "    \"\"\"Check if answer addresses the question.\"\"\"\n",
    "    chain_of_thought: str = Field(\n",
    "        ..., description=\"Step by step reasoning to check if the answer addresses the questions\"\n",
    "    )\n",
    "    is_useful: bool = Field(\n",
    "        description=\"Answer addresses the question\"\n",
    "    )\n",
    "\n",
    "answer_grader_llm = llm.with_structured_output(AnswerGrade, method=\"function_calling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9db6437e-b9c5-40ff-8244-2cb81f27c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"Question optimization for search.\"\"\"\n",
    "    chain_of_thought: str = Field(\n",
    "        ..., description=\"Step by step reasoning to optimize query for search\"\n",
    "    )\n",
    "    search_query: str = Field(\n",
    "        description=\"Optimized search query\"\n",
    "    )\n",
    "\n",
    "search_llm = llm.with_structured_output(SearchQuery, method=\"function_calling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba910aa4-c2e7-4274-82b0-e9a24fd88913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]) -> list[str]:\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9d13f6e-342d-4a51-87c7-a42dd91c9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    retrieval_grade: RetrievalGrade\n",
    "    documents: list[Document]\n",
    "    relevance_grades: list[RelevanceGrade]\n",
    "    generation: str\n",
    "    hallucinations_grade: HallucationsGrade\n",
    "    context: list[Document]\n",
    "    answer_grade: AnswerGrade\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4330b73a-e93a-44a8-9671-92af99eae231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_retrieval(state: State):\n",
    "    question = state[\"question\"]\n",
    "    retrieval_prompt = retrieval_prompt_template.format(question=question)\n",
    "    retrieval_grade = retrieval_grader_llm.invoke(retrieval_prompt)\n",
    "    return {\"retrieval_grade\": retrieval_grade}\n",
    "\n",
    "\n",
    "def decide_to_retrieve(state: State) -> Literal[\"retrieve\", \"generate_answer\"]:\n",
    "    retrieval_grade = state[\"retrieval_grade\"]\n",
    "    \n",
    "    if retrieval_grade.is_required:\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        return \"generate_answer\"\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def grade_documents(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    relevance_grades = grade_document_relevance.batch(\n",
    "        documents, question=question\n",
    "    )\n",
    "    filtered_documents = [document for (document, relevance_grade) in zip(documents, relevance_grades) if relevance_grade.is_relevant]\n",
    "            \n",
    "    return {\"context\": filtered_documents, \"relevance_grades\": relevance_grades}\n",
    "\n",
    "    \n",
    "def check_documents_relevance(state: State) -> Literal[\"generate_rag_answer\", \"generate_no_answer\"]:\n",
    "    filtered_documents = state[\"context\"]\n",
    "    \n",
    "    if len(filtered_documents) > 0:\n",
    "        return \"generate_rag_answer\"\n",
    "    else:\n",
    "        return \"generate_no_answer\"\n",
    "        \n",
    "\n",
    "def generate_rag_answer(state: State):\n",
    "    docs_content = format_docs(state[\"context\"])\n",
    "    rag_prompt = rag_prompt_template.format(\n",
    "        question=state[\"question\"],\n",
    "        context=docs_content\n",
    "    )\n",
    "    response = llm.invoke([\n",
    "        HumanMessage(content=rag_prompt)\n",
    "    ])\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "def generate_answer(state: State):\n",
    "    answer_prompt = answer_prompt_template.format(\n",
    "        question=state[\"question\"]\n",
    "    )\n",
    "    response = llm.invoke([\n",
    "        HumanMessage(content=answer_prompt)\n",
    "    ])\n",
    "    return {\"answer\": response.content}\n",
    "    \n",
    "\n",
    "def generate_no_answer(state: State):\n",
    "    return {\"answer\": no_answer_prompt}\n",
    "    \n",
    "\n",
    "def grade_hallucinations(state: State):\n",
    "    filtered_documents = state[\"context\"]\n",
    "    answer = state[\"answer\"]\n",
    "    hallucinations_grading_prompt = hallucinations_grading_prompt_template.format(\n",
    "        context=filtered_documents,\n",
    "        answer=answer\n",
    "    )\n",
    "    hallucinations_grade = hallucations_grader_llm.invoke(hallucinations_grading_prompt)\n",
    "    return {\"hallucinations_grade\": hallucinations_grade}\n",
    "\n",
    "\n",
    "def check_hallucinations(state: State) -> Literal[\"grade_answer\", \"generate_rag_answer\"]:\n",
    "    hallucinations_grade = state[\"hallucinations_grade\"]\n",
    "    \n",
    "    if hallucinations_grade.is_grounded:\n",
    "        return \"grade_answer\"\n",
    "    else:\n",
    "        return \"generate_rag_answer\"\n",
    "\n",
    "\n",
    "def grade_answer(state: State):\n",
    "    question = state[\"question\"]\n",
    "    answer = state[\"answer\"]\n",
    "    answer_grading_prompt = answer_grading_prompt_template.format(\n",
    "        question=question,\n",
    "        answer=answer\n",
    "    )\n",
    "    answer_grade = answer_grader_llm.invoke(answer_grading_prompt)\n",
    "    return {\"answer_grade\": answer_grade}\n",
    "\n",
    "\n",
    "def check_answer(state: State) -> Literal[\"__end__\", \"rewrite_query\"]:\n",
    "    answer_grade = state[\"answer_grade\"]\n",
    "    \n",
    "    if answer_grade.is_useful:\n",
    "        return END\n",
    "    else:\n",
    "        return \"rewrite_query\"\n",
    "\n",
    "\n",
    "def rewrite_query(state: State):\n",
    "    question = state[\"question\"]\n",
    "    query_rewriting_prompt = query_rewriting_prompt_template.format(question=question)\n",
    "    response = search_llm.invoke(query_rewriting_prompt)\n",
    "    return {\"question\": response.search_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ead6e91-8dd8-45a6-a79a-0fab3fa8c36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7f4e884a8490>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_edge(START, \"grade_retrieval\")\n",
    "graph_builder.add_node(\"grade_retrieval\", grade_retrieval)\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_retrieval\", \n",
    "    decide_to_retrieve,\n",
    "    [\"retrieve\", \"generate_answer\"]\n",
    ")\n",
    "\n",
    "graph_builder.add_node(\"generate_answer\", generate_answer)\n",
    "graph_builder.add_edge(\"generate_answer\", END)\n",
    "\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_edge(\"retrieve\", \"grade_documents\")\n",
    "graph_builder.add_node(\"grade_documents\", grade_documents)\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_documents\", \n",
    "    check_documents_relevance,\n",
    "    [\"generate_rag_answer\", \"generate_no_answer\"]\n",
    ")\n",
    "\n",
    "graph_builder.add_node(\"generate_rag_answer\", generate_rag_answer)\n",
    "graph_builder.add_edge(\"generate_rag_answer\", \"grade_hallucinations\")\n",
    "graph_builder.add_node(\"grade_hallucinations\", grade_hallucinations)\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_hallucinations\", \n",
    "    check_hallucinations,\n",
    "    [\"grade_answer\", \"generate_rag_answer\"]\n",
    ")\n",
    "\n",
    "graph_builder.add_node(\"generate_no_answer\", generate_no_answer)\n",
    "graph_builder.add_edge(\"generate_no_answer\", END)\n",
    "\n",
    "graph_builder.add_node(\"grade_answer\", grade_answer)\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"grade_answer\", \n",
    "    check_answer,\n",
    "    [\"__end__\", \"rewrite_query\"]\n",
    ")\n",
    "\n",
    "graph_builder.add_node(\"rewrite_query\", rewrite_query)\n",
    "graph_builder.add_edge(\"rewrite_query\", \"retrieve\")\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f12ae9ea-a596-4136-93f7-551f6933f8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What are common types of agent memory?'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'retrieval_grade'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RetrievalGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The user asks about common types of agent memory. Typically, agent memory types include </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">episodic memory, semantic memory, working memory, and long-term memory, among others. These are standard concepts </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">likely to be included in general knowledge or documents about agents. If the available data covers basic AI or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">agent memory types, no additional retrieval is needed. Without evidence suggesting a lack of these details, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval is not required.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">is_required</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Common types of agent memory include:\\n\\n1. **Short-term memory (Working Memory):** Used to store </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information temporarily for immediate processing and decision-making. It holds recent sensory inputs or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">intermediate results.\\n\\n2. **Long-term memory:** Stores knowledge, experiences, and learned behaviors that the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">agent can retrieve and use over extended periods.\\n\\n3. **Episodic memory:** Stores specific experiences or events </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that the agent has encountered, enabling the agent to recall past states or situations.\\n\\n4. **Semantic memory:** </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Contains general facts, concepts, and information about the world that the agent has learned but is not tied to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific experiences.\\n\\n5. **Procedural memory:** Holds knowledge about how to perform tasks or actions, such as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">skills or routines that the agent can execute automatically.\\n\\nThese memory types help agents to perceive their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">environment, learn from it, and make informed decisions.'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'question'\u001b[0m: \u001b[32m'What are common types of agent memory?'\u001b[0m,\n",
       "    \u001b[32m'retrieval_grade'\u001b[0m: \u001b[1;35mRetrievalGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The user asks about common types of agent memory. Typically, agent memory types include \u001b[0m\n",
       "\u001b[32mepisodic memory, semantic memory, working memory, and long-term memory, among others. These are standard concepts \u001b[0m\n",
       "\u001b[32mlikely to be included in general knowledge or documents about agents. If the available data covers basic AI or \u001b[0m\n",
       "\u001b[32magent memory types, no additional retrieval is needed. Without evidence suggesting a lack of these details, \u001b[0m\n",
       "\u001b[32mretrieval is not required.'\u001b[0m,\n",
       "        \u001b[33mis_required\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'answer'\u001b[0m: \u001b[32m'Common types of agent memory include:\\n\\n1. **Short-term memory \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWorking Memory\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:** Used to store \u001b[0m\n",
       "\u001b[32minformation temporarily for immediate processing and decision-making. It holds recent sensory inputs or \u001b[0m\n",
       "\u001b[32mintermediate results.\\n\\n2. **Long-term memory:** Stores knowledge, experiences, and learned behaviors that the \u001b[0m\n",
       "\u001b[32magent can retrieve and use over extended periods.\\n\\n3. **Episodic memory:** Stores specific experiences or events \u001b[0m\n",
       "\u001b[32mthat the agent has encountered, enabling the agent to recall past states or situations.\\n\\n4. **Semantic memory:** \u001b[0m\n",
       "\u001b[32mContains general facts, concepts, and information about the world that the agent has learned but is not tied to \u001b[0m\n",
       "\u001b[32mspecific experiences.\\n\\n5. **Procedural memory:** Holds knowledge about how to perform tasks or actions, such as \u001b[0m\n",
       "\u001b[32mskills or routines that the agent can execute automatically.\\n\\nThese memory types help agents to perceive their \u001b[0m\n",
       "\u001b[32menvironment, learn from it, and make informed decisions.'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Common types of agent memory include:                                                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">Short-term memory (Working Memory):</span> Used to store information temporarily for immediate processing and          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>decision-making. It holds recent sensory inputs or intermediate results.                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Long-term memory:</span> Stores knowledge, experiences, and learned behaviors that the agent can retrieve and use over \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>extended periods.                                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">Episodic memory:</span> Stores specific experiences or events that the agent has encountered, enabling the agent to    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>recall past states or situations.                                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">Semantic memory:</span> Contains general facts, concepts, and information about the world that the agent has learned   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>but is not tied to specific experiences.                                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 5 </span><span style=\"font-weight: bold\">Procedural memory:</span> Holds knowledge about how to perform tasks or actions, such as skills or routines that the   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>agent can execute automatically.                                                                                \n",
       "\n",
       "These memory types help agents to perceive their environment, learn from it, and make informed decisions.          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Common types of agent memory include:                                                                              \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mShort-term memory (Working Memory):\u001b[0m Used to store information temporarily for immediate processing and          \n",
       "\u001b[1;33m   \u001b[0mdecision-making. It holds recent sensory inputs or intermediate results.                                        \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mLong-term memory:\u001b[0m Stores knowledge, experiences, and learned behaviors that the agent can retrieve and use over \n",
       "\u001b[1;33m   \u001b[0mextended periods.                                                                                               \n",
       "\u001b[1;33m 3 \u001b[0m\u001b[1mEpisodic memory:\u001b[0m Stores specific experiences or events that the agent has encountered, enabling the agent to    \n",
       "\u001b[1;33m   \u001b[0mrecall past states or situations.                                                                               \n",
       "\u001b[1;33m 4 \u001b[0m\u001b[1mSemantic memory:\u001b[0m Contains general facts, concepts, and information about the world that the agent has learned   \n",
       "\u001b[1;33m   \u001b[0mbut is not tied to specific experiences.                                                                        \n",
       "\u001b[1;33m 5 \u001b[0m\u001b[1mProcedural memory:\u001b[0m Holds knowledge about how to perform tasks or actions, such as skills or routines that the   \n",
       "\u001b[1;33m   \u001b[0magent can execute automatically.                                                                                \n",
       "\n",
       "These memory types help agents to perceive their environment, learn from it, and make informed decisions.          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What are common types of agent memory?\"\n",
    "\n",
    "response = graph.invoke({\"question\": query})\n",
    "rprint(response)\n",
    "rprint(Markdown(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a4cc058-ed9e-4a91-8d6f-593dfd7f6cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What are recent types of adversarial attacks in LLM?'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'retrieval_grade'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RetrievalGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'User is asking about recent types of adversarial attacks on large language models (LLMs).</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">To answer this, specific and up-to-date information about various adversarial attacks targeting LLMs is required. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Since the question explicitly asks for recent types, this implies a need for the latest research findings or news, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">which might not be contained in default knowledge. Therefore, additional document retrieval is likely necessary to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">provide a comprehensive and current answer.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">is_required</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'documents'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'699566fc-cb81-4747-bc34-a1e96753b4cc'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Weng, Lilian. (Oct 2023). “Adversarial Attacks on LLMs”. Lil’Log. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'b27ccb01-34d4-4f43-8bdc-243fc00cb01f'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Adversarial Attacks on LLMs\\n    \\nDate: October 25, 2023  |  Estimated Reading Time: 33 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">min  |  Author: Lilian Weng'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'b882f510-aed2-489e-9ad0-50f0e574ea51'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Or\\n@article{weng2023attack,\\n  title   = \"Adversarial Attacks on LLMs\",\\n  author  = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Oct\",\\n  url     = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\\n}\\nReferences#\\n[1] Madry et al. “Towards Deep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Learning Models Resistant to Adversarial Attacks”. ICLR 2018.\\n[2] Ribeiro et al. “Semantically equivalent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">adversarial rules for debugging NLP models”. ACL 2018.\\n[3] Guo et al. “Gradient-based adversarial attacks against </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">text transformers”. arXiv preprint arXiv:2104.13733 (2021).\\n[4] Ebrahimi et al. “HotFlip: White-Box Adversarial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Examples for Text Classification”. ACL 2018.\\n[5] Wallace et al. “Universal Adversarial Triggers for Attacking and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Analyzing NLP.” EMNLP-IJCNLP 2019. | code\\n[6] Mehrabi et al. “Robust Conversational Agents against Imperceptible </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Toxicity Triggers.” NAACL 2022.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'e5967c3a-5ad9-4182-98ce-08c8fd5e7e03'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Fig. 1. An overview of threats to LLM-based applications. (Image source: Greshake et al. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2023)\\nClassification#\\nAdversarial attacks on classifiers have attracted more attention in the research community </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in the past, many in the image domain. LLMs can be used for classification too. Given an input $\\\\mathbf{x}$ and a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">classifier $f(.)$, we would like to find an adversarial version of the input, denoted as $\\\\mathbf{x}_\\\\text{adv}$,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with imperceptible difference from $\\\\mathbf{x}$, such that $f(\\\\mathbf{x}) \\\\neq </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">f(\\\\mathbf{x}_\\\\text{adv})$.\\nText Generation#'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'relevance_grades'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelevanceGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The user question asks about recent types of adversarial attacks in large language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models (LLMs). The document retrieved is titled \"Adversarial Attacks on LLMs\" and was published recently in October</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2023. This document likely discusses adversarial attack methods related to LLMs, which directly matches the focus </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of the user question. Therefore, the document is relevant for answering the user question.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelevanceGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The user's question is about recent types of adversarial attacks in large language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models (LLMs). The retrieved document's title is 'Adversarial Attacks on LLMs' and it is authored recently (October</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">25, 2023). This strongly suggests that the document discusses adversarial attacks on LLMs, which likely includes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recent types of such attacks. Therefore, the document is relevant to the user's question.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelevanceGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The user question asks about recent types of adversarial attacks in large language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models (LLM). The retrieved document is an article titled 'Adversarial Attacks on LLMs' by Lilian Weng, published </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in 2023. The document appears to be focused on adversarial attacks in LLMs and even includes references to relevant</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">literature in the area such as works on deep learning models resistant to adversarial attacks, adversarial rules </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">for NLP models, gradient-based adversarial attacks against text transformers, and more. This context suggests the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document is highly relevant to the user's question since it specifically addresses adversarial attacks on LLMs and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is recent (2023). Therefore, the document is relevant.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelevanceGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The user question asks about recent types of adversarial attacks in LLMs. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document mentions adversarial attacks specifically in the context of LLM-based applications and discusses the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">concept of adversarial attacks on classifiers, including LLMs used for classification. It also elaborates on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generating adversarial inputs to fool classifiers. This shows the document is directly addressing adversarial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attacks related to LLMs, making it relevant to the question asked.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'hallucinations_grade'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HallucationsGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The answer summarizes types of adversarial attacks on large language models (LLMs) based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on Lilian Weng\\'s October 2023 article \"Adversarial Attacks on LLMs\". The facts include a citation to this article,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the name of the author, the date, and the URL, all confirming that the source is trustworthy and authoritative. The</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">detailed attacks listed in the answer correspond to the references cited in the facts: Ribeiro et al. (2018), Guo </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al. (2021), Ebrahimi et al. (2018), Wallace et al. (2019), Mehrabi et al. (2022), all explicitly mentioned in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the retrieved documents. The answer correctly describes classification adversarial attacks with the notation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\(f(\\\\mathbf{x}) \\\\neq f(\\\\mathbf{x}_\\\\text{adv})\\\\) which matches the fact snippet describing adversarial attacks</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on classifiers from Lilian Weng\\'s article. The answer\\'s categories and examples are well aligned with the factual</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">references listed. There is no contradictory information or hallucinated claims outside those references. Hence, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the answer is grounded in the set of facts provided.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">is_grounded</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'699566fc-cb81-4747-bc34-a1e96753b4cc'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Weng, Lilian. (Oct 2023). “Adversarial Attacks on LLMs”. Lil’Log. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'b27ccb01-34d4-4f43-8bdc-243fc00cb01f'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Adversarial Attacks on LLMs\\n    \\nDate: October 25, 2023  |  Estimated Reading Time: 33 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">min  |  Author: Lilian Weng'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'b882f510-aed2-489e-9ad0-50f0e574ea51'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Or\\n@article{weng2023attack,\\n  title   = \"Adversarial Attacks on LLMs\",\\n  author  = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Oct\",\\n  url     = </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\\n}\\nReferences#\\n[1] Madry et al. “Towards Deep </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Learning Models Resistant to Adversarial Attacks”. ICLR 2018.\\n[2] Ribeiro et al. “Semantically equivalent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">adversarial rules for debugging NLP models”. ACL 2018.\\n[3] Guo et al. “Gradient-based adversarial attacks against </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">text transformers”. arXiv preprint arXiv:2104.13733 (2021).\\n[4] Ebrahimi et al. “HotFlip: White-Box Adversarial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Examples for Text Classification”. ACL 2018.\\n[5] Wallace et al. “Universal Adversarial Triggers for Attacking and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Analyzing NLP.” EMNLP-IJCNLP 2019. | code\\n[6] Mehrabi et al. “Robust Conversational Agents against Imperceptible </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Toxicity Triggers.” NAACL 2022.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'e5967c3a-5ad9-4182-98ce-08c8fd5e7e03'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Fig. 1. An overview of threats to LLM-based applications. (Image source: Greshake et al. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2023)\\nClassification#\\nAdversarial attacks on classifiers have attracted more attention in the research community </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in the past, many in the image domain. LLMs can be used for classification too. Given an input $\\\\mathbf{x}$ and a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">classifier $f(.)$, we would like to find an adversarial version of the input, denoted as $\\\\mathbf{x}_\\\\text{adv}$,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with imperceptible difference from $\\\\mathbf{x}$, such that $f(\\\\mathbf{x}) \\\\neq </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">f(\\\\mathbf{x}_\\\\text{adv})$.\\nText Generation#'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'answer_grade'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AnswerGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The question asks for recent types of adversarial attacks in large language models </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(LLMs). The answer provides a detailed, categorized list of different attack types specifically referencing a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recent source from October 2023. It includes a variety of attack types such as adversarial attacks on classifiers, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">gradient-based attacks, white-box token-level attacks, universal adversarial triggers, semantically equivalent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">adversarial rules, imperceptible or subtle toxic triggers, and attacks targeting text generation. The answer also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">summarizes these types and references a credible source for further reading. The response is thorough, relevant, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and clearly addresses the question fully.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">is_useful</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Based on Lilian Weng’s October 2023 article *“Adversarial Attacks on LLMs”*, recent types of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">adversarial attacks on large language models (LLMs) can be broadly categorized as follows:\\n\\n1. **Adversarial </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attacks on classifiers**:  \\n   These attacks attempt to slightly perturb an input text \\\\(\\\\mathbf{x}\\\\) to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">produce an adversarial example \\\\(\\\\mathbf{x}_\\\\text{adv}\\\\) that causes the model’s classifier \\\\(f(.)\\\\) to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">change its prediction, i.e., \\\\(f(\\\\mathbf{x}) \\\\neq f(\\\\mathbf{x}_\\\\text{adv})\\\\), while keeping the modifications</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">imperceptible or semantically similar to humans. This category is inspired by classical adversarial attacks widely </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">studied in image classification and adapted to text classification and LLMs functioning as classifiers.\\n\\n2. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">**Gradient-based attacks**:  \\n   Methods that use the model gradients to guide perturbations in the text input to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">mislead the model outputs, such as the work by Guo et al. (2021) [3].\\n\\n3. **White-box attacks at the token level </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(e.g., HotFlip)**:  \\n   White-box attacks that flip tokens based on gradient information to change the model's </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prediction with minimal edits introduced to the text, exemplified by Ebrahimi et al. (2018) [4].\\n\\n4. **Universal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">adversarial triggers**:  \\n   These are input triggers—specific sequences of tokens—that, when prepended or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">appended to any input, can consistently cause the model to produce incorrect or undesired outputs, as shown by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Wallace et al. (2019) [5].\\n\\n5. **Semantically equivalent adversarial rules**:  \\n   Creating adversarial examples</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">by applying transformations that preserve semantic meaning but fool the model, following approaches like Ribeiro et</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al. (2018) [2].\\n\\n6. **Imperceptible or subtle toxic triggers**:  \\n   Tiny perturbations designed to inject toxic</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or harmful content into otherwise safe prompts, as studied by Mehrabi et al. (2022) [6].\\n\\n7. **Attacks targeting </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">text generation** (implied but less explicitly described):  \\n   Attackers manipulate prompts or inputs to generate</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">harmful, biased, or misleading outputs from LLMs.\\n\\n---\\n\\n**Summary:**  \\nRecent adversarial attacks on LLMs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">include gradient-based token perturbations, white-box token flips (HotFlip), universal adversarial triggers that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">work across inputs, semantically preserving adversarial substitutions, and imperceptible toxicity injections. These</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">strategies exploit LLM vulnerabilities both in classification and generation tasks to cause undesired output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">behaviors.\\n\\nFor full details, see the referenced article:  \\nWeng, Lilian. (Oct 2023). “Adversarial Attacks on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLMs”. Lil’Log. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'question'\u001b[0m: \u001b[32m'What are recent types of adversarial attacks in LLM?'\u001b[0m,\n",
       "    \u001b[32m'retrieval_grade'\u001b[0m: \u001b[1;35mRetrievalGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'User is asking about recent types of adversarial attacks on large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\u001b[0m\n",
       "\u001b[32mTo answer this, specific and up-to-date information about various adversarial attacks targeting LLMs is required. \u001b[0m\n",
       "\u001b[32mSince the question explicitly asks for recent types, this implies a need for the latest research findings or news, \u001b[0m\n",
       "\u001b[32mwhich might not be contained in default knowledge. Therefore, additional document retrieval is likely necessary to \u001b[0m\n",
       "\u001b[32mprovide a comprehensive and current answer.'\u001b[0m,\n",
       "        \u001b[33mis_required\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'documents'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'699566fc-cb81-4747-bc34-a1e96753b4cc'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Weng, Lilian. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOct 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. “Adversarial Attacks on LLMs”. Lil’Log. \u001b[0m\n",
       "\u001b[32mhttps://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'b27ccb01-34d4-4f43-8bdc-243fc00cb01f'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Adversarial Attacks on LLMs\\n    \\nDate: October 25, 2023  |  Estimated Reading Time: 33 \u001b[0m\n",
       "\u001b[32mmin  |  Author: Lilian Weng'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'b882f510-aed2-489e-9ad0-50f0e574ea51'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Or\\n@article\u001b[0m\u001b[32m{\u001b[0m\u001b[32mweng2023attack,\\n  title   = \"Adversarial Attacks on LLMs\",\\n  author  = \u001b[0m\n",
       "\u001b[32m\"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Oct\",\\n  url     = \u001b[0m\n",
       "\u001b[32m\"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nReferences#\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Madry et al. “Towards Deep \u001b[0m\n",
       "\u001b[32mLearning Models Resistant to Adversarial Attacks”. ICLR 2018.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Ribeiro et al. “Semantically equivalent \u001b[0m\n",
       "\u001b[32madversarial rules for debugging NLP models”. ACL 2018.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Guo et al. “Gradient-based adversarial attacks against \u001b[0m\n",
       "\u001b[32mtext transformers”. arXiv preprint arXiv:2104.13733 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2021\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Ebrahimi et al. “HotFlip: White-Box Adversarial \u001b[0m\n",
       "\u001b[32mExamples for Text Classification”. ACL 2018.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Wallace et al. “Universal Adversarial Triggers for Attacking and \u001b[0m\n",
       "\u001b[32mAnalyzing NLP.” EMNLP-IJCNLP 2019. | code\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Mehrabi et al. “Robust Conversational Agents against Imperceptible \u001b[0m\n",
       "\u001b[32mToxicity Triggers.” NAACL 2022.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'e5967c3a-5ad9-4182-98ce-08c8fd5e7e03'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Fig. 1. An overview of threats to LLM-based applications. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Greshake et al. \u001b[0m\n",
       "\u001b[32m2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nClassification#\\nAdversarial attacks on classifiers have attracted more attention in the research community \u001b[0m\n",
       "\u001b[32min the past, many in the image domain. LLMs can be used for classification too. Given an input $\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$ and a \u001b[0m\n",
       "\u001b[32mclassifier $f\u001b[0m\u001b[32m(\u001b[0m\u001b[32m.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$, we would like to find an adversarial version of the input, denoted as $\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_\\\\text\u001b[0m\u001b[32m{\u001b[0m\u001b[32madv\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$,\u001b[0m\n",
       "\u001b[32mwith imperceptible difference from $\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$, such that $f\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\\\neq \u001b[0m\n",
       "\u001b[32mf\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_\\\\text\u001b[0m\u001b[32m{\u001b[0m\u001b[32madv\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$.\\nText Generation#'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'relevance_grades'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mRelevanceGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The user question asks about recent types of adversarial attacks in large language \u001b[0m\n",
       "\u001b[32mmodels \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The document retrieved is titled \"Adversarial Attacks on LLMs\" and was published recently in October\u001b[0m\n",
       "\u001b[32m2023. This document likely discusses adversarial attack methods related to LLMs, which directly matches the focus \u001b[0m\n",
       "\u001b[32mof the user question. Therefore, the document is relevant for answering the user question.'\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mRelevanceGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m user's question is about recent types of adversarial attacks in large language \u001b[0m\n",
       "\u001b[32mmodels \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The retrieved document's title is 'Adversarial Attacks on LLMs' and it is authored recently \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOctober\u001b[0m\n",
       "\u001b[32m25, 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This strongly suggests that the document discusses adversarial attacks on LLMs, which likely includes \u001b[0m\n",
       "\u001b[32mrecent types of such attacks. Therefore, the document is relevant to the user's question.\"\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mRelevanceGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m user question asks about recent types of adversarial attacks in large language \u001b[0m\n",
       "\u001b[32mmodels \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The retrieved document is an article titled 'Adversarial Attacks on LLMs' by Lilian Weng, published \u001b[0m\n",
       "\u001b[32min 2023. The document appears to be focused on adversarial attacks in LLMs and even includes references to relevant\u001b[0m\n",
       "\u001b[32mliterature in the area such as works on deep learning models resistant to adversarial attacks, adversarial rules \u001b[0m\n",
       "\u001b[32mfor NLP models, gradient-based adversarial attacks against text transformers, and more. This context suggests the \u001b[0m\n",
       "\u001b[32mdocument is highly relevant to the user's question since it specifically addresses adversarial attacks on LLMs and \u001b[0m\n",
       "\u001b[32mis recent \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Therefore, the document is relevant.\"\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mRelevanceGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The user question asks about recent types of adversarial attacks in LLMs. The \u001b[0m\n",
       "\u001b[32mdocument mentions adversarial attacks specifically in the context of LLM-based applications and discusses the \u001b[0m\n",
       "\u001b[32mconcept of adversarial attacks on classifiers, including LLMs used for classification. It also elaborates on \u001b[0m\n",
       "\u001b[32mgenerating adversarial inputs to fool classifiers. This shows the document is directly addressing adversarial \u001b[0m\n",
       "\u001b[32mattacks related to LLMs, making it relevant to the question asked.'\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'hallucinations_grade'\u001b[0m: \u001b[1;35mHallucationsGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The answer summarizes types of adversarial attacks on large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based \u001b[0m\n",
       "\u001b[32mon Lilian Weng\\'s October 2023 article \"Adversarial Attacks on LLMs\". The facts include a citation to this article,\u001b[0m\n",
       "\u001b[32mthe name of the author, the date, and the URL, all confirming that the source is trustworthy and authoritative. The\u001b[0m\n",
       "\u001b[32mdetailed attacks listed in the answer correspond to the references cited in the facts: Ribeiro et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Guo \u001b[0m\n",
       "\u001b[32met al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2021\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Ebrahimi et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Wallace et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Mehrabi et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2022\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, all explicitly mentioned in \u001b[0m\n",
       "\u001b[32mthe retrieved documents. The answer correctly describes classification adversarial attacks with the notation \u001b[0m\n",
       "\u001b[32m\\\\\u001b[0m\u001b[32m(\u001b[0m\u001b[32mf\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\\\neq f\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_\\\\text\u001b[0m\u001b[32m{\u001b[0m\u001b[32madv\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\\\\u001b[0m\u001b[32m)\u001b[0m\u001b[32m which matches the fact snippet describing adversarial attacks\u001b[0m\n",
       "\u001b[32mon classifiers from Lilian Weng\\'s article. The answer\\'s categories and examples are well aligned with the factual\u001b[0m\n",
       "\u001b[32mreferences listed. There is no contradictory information or hallucinated claims outside those references. Hence, \u001b[0m\n",
       "\u001b[32mthe answer is grounded in the set of facts provided.'\u001b[0m,\n",
       "        \u001b[33mis_grounded\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'context'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'699566fc-cb81-4747-bc34-a1e96753b4cc'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Weng, Lilian. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOct 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. “Adversarial Attacks on LLMs”. Lil’Log. \u001b[0m\n",
       "\u001b[32mhttps://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'b27ccb01-34d4-4f43-8bdc-243fc00cb01f'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Adversarial Attacks on LLMs\\n    \\nDate: October 25, 2023  |  Estimated Reading Time: 33 \u001b[0m\n",
       "\u001b[32mmin  |  Author: Lilian Weng'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'b882f510-aed2-489e-9ad0-50f0e574ea51'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Or\\n@article\u001b[0m\u001b[32m{\u001b[0m\u001b[32mweng2023attack,\\n  title   = \"Adversarial Attacks on LLMs\",\\n  author  = \u001b[0m\n",
       "\u001b[32m\"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Oct\",\\n  url     = \u001b[0m\n",
       "\u001b[32m\"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nReferences#\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Madry et al. “Towards Deep \u001b[0m\n",
       "\u001b[32mLearning Models Resistant to Adversarial Attacks”. ICLR 2018.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Ribeiro et al. “Semantically equivalent \u001b[0m\n",
       "\u001b[32madversarial rules for debugging NLP models”. ACL 2018.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Guo et al. “Gradient-based adversarial attacks against \u001b[0m\n",
       "\u001b[32mtext transformers”. arXiv preprint arXiv:2104.13733 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2021\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Ebrahimi et al. “HotFlip: White-Box Adversarial \u001b[0m\n",
       "\u001b[32mExamples for Text Classification”. ACL 2018.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Wallace et al. “Universal Adversarial Triggers for Attacking and \u001b[0m\n",
       "\u001b[32mAnalyzing NLP.” EMNLP-IJCNLP 2019. | code\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Mehrabi et al. “Robust Conversational Agents against Imperceptible \u001b[0m\n",
       "\u001b[32mToxicity Triggers.” NAACL 2022.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'e5967c3a-5ad9-4182-98ce-08c8fd5e7e03'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Fig. 1. An overview of threats to LLM-based applications. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Greshake et al. \u001b[0m\n",
       "\u001b[32m2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nClassification#\\nAdversarial attacks on classifiers have attracted more attention in the research community \u001b[0m\n",
       "\u001b[32min the past, many in the image domain. LLMs can be used for classification too. Given an input $\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$ and a \u001b[0m\n",
       "\u001b[32mclassifier $f\u001b[0m\u001b[32m(\u001b[0m\u001b[32m.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$, we would like to find an adversarial version of the input, denoted as $\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_\\\\text\u001b[0m\u001b[32m{\u001b[0m\u001b[32madv\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$,\u001b[0m\n",
       "\u001b[32mwith imperceptible difference from $\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$, such that $f\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\\\neq \u001b[0m\n",
       "\u001b[32mf\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_\\\\text\u001b[0m\u001b[32m{\u001b[0m\u001b[32madv\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$.\\nText Generation#'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'answer_grade'\u001b[0m: \u001b[1;35mAnswerGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The question asks for recent types of adversarial attacks in large language models \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The answer provides a detailed, categorized list of different attack types specifically referencing a \u001b[0m\n",
       "\u001b[32mrecent source from October 2023. It includes a variety of attack types such as adversarial attacks on classifiers, \u001b[0m\n",
       "\u001b[32mgradient-based attacks, white-box token-level attacks, universal adversarial triggers, semantically equivalent \u001b[0m\n",
       "\u001b[32madversarial rules, imperceptible or subtle toxic triggers, and attacks targeting text generation. The answer also \u001b[0m\n",
       "\u001b[32msummarizes these types and references a credible source for further reading. The response is thorough, relevant, \u001b[0m\n",
       "\u001b[32mand clearly addresses the question fully.'\u001b[0m,\n",
       "        \u001b[33mis_useful\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'answer'\u001b[0m: \u001b[32m\"Based on Lilian Weng’s October 2023 article *“Adversarial Attacks on LLMs”*, recent types of \u001b[0m\n",
       "\u001b[32madversarial attacks on large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m can be broadly categorized as follows:\\n\\n1. **Adversarial \u001b[0m\n",
       "\u001b[32mattacks on classifiers**:  \\n   These attacks attempt to slightly perturb an input text \\\\\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\\\\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to \u001b[0m\n",
       "\u001b[32mproduce an adversarial example \\\\\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_\\\\text\u001b[0m\u001b[32m{\u001b[0m\u001b[32madv\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\\\\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that causes the model’s classifier \\\\\u001b[0m\u001b[32m(\u001b[0m\u001b[32mf\u001b[0m\u001b[32m(\u001b[0m\u001b[32m.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\\\\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to \u001b[0m\n",
       "\u001b[32mchange its prediction, i.e., \\\\\u001b[0m\u001b[32m(\u001b[0m\u001b[32mf\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\\\neq f\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_\\\\text\u001b[0m\u001b[32m{\u001b[0m\u001b[32madv\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\\\\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, while keeping the modifications\u001b[0m\n",
       "\u001b[32mimperceptible or semantically similar to humans. This category is inspired by classical adversarial attacks widely \u001b[0m\n",
       "\u001b[32mstudied in image classification and adapted to text classification and LLMs functioning as classifiers.\\n\\n2. \u001b[0m\n",
       "\u001b[32m**Gradient-based attacks**:  \\n   Methods that use the model gradients to guide perturbations in the text input to \u001b[0m\n",
       "\u001b[32mmislead the model outputs, such as the work by Guo et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2021\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\n3. **White-box attacks at the token level \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32me.g., HotFlip\u001b[0m\u001b[32m)\u001b[0m\u001b[32m**:  \\n   White-box attacks that flip tokens based on gradient information to change the model's \u001b[0m\n",
       "\u001b[32mprediction with minimal edits introduced to the text, exemplified by Ebrahimi et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\n4. **Universal \u001b[0m\n",
       "\u001b[32madversarial triggers**:  \\n   These are input triggers—specific sequences of tokens—that, when prepended or \u001b[0m\n",
       "\u001b[32mappended to any input, can consistently cause the model to produce incorrect or undesired outputs, as shown by \u001b[0m\n",
       "\u001b[32mWallace et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\n5. **Semantically equivalent adversarial rules**:  \\n   Creating adversarial examples\u001b[0m\n",
       "\u001b[32mby applying transformations that preserve semantic meaning but fool the model, following approaches like Ribeiro et\u001b[0m\n",
       "\u001b[32mal. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\n6. **Imperceptible or subtle toxic triggers**:  \\n   Tiny perturbations designed to inject toxic\u001b[0m\n",
       "\u001b[32mor harmful content into otherwise safe prompts, as studied by Mehrabi et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2022\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\n7. **Attacks targeting \u001b[0m\n",
       "\u001b[32mtext generation** \u001b[0m\u001b[32m(\u001b[0m\u001b[32mimplied but less explicitly described\u001b[0m\u001b[32m)\u001b[0m\u001b[32m:  \\n   Attackers manipulate prompts or inputs to generate\u001b[0m\n",
       "\u001b[32mharmful, biased, or misleading outputs from LLMs.\\n\\n---\\n\\n**Summary:**  \\nRecent adversarial attacks on LLMs \u001b[0m\n",
       "\u001b[32minclude gradient-based token perturbations, white-box token flips \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotFlip\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, universal adversarial triggers that \u001b[0m\n",
       "\u001b[32mwork across inputs, semantically preserving adversarial substitutions, and imperceptible toxicity injections. These\u001b[0m\n",
       "\u001b[32mstrategies exploit LLM vulnerabilities both in classification and generation tasks to cause undesired output \u001b[0m\n",
       "\u001b[32mbehaviors.\\n\\nFor full details, see the referenced article:  \\nWeng, Lilian. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOct 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. “Adversarial Attacks on \u001b[0m\n",
       "\u001b[32mLLMs”. Lil’Log. https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on Lilian Weng’s October 2023 article <span style=\"font-style: italic\">“Adversarial Attacks on LLMs”</span>, recent types of adversarial attacks on  \n",
       "large language models (LLMs) can be broadly categorized as follows:                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">Adversarial attacks on classifiers</span>:                                                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>These attacks attempt to slightly perturb an input text (\\mathbf{x}) to produce an adversarial example          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>(\\mathbf{x}<span style=\"font-style: italic\">\\text{adv}) that causes the model’s classifier (f(.)) to change its prediction, i.e., (f(\\mathbf{x}) </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"font-style: italic\">\\neq f(\\mathbf{x}</span>\\text{adv})), while keeping the modifications imperceptible or semantically similar to humans. \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>This category is inspired by classical adversarial attacks widely studied in image classification and adapted to\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>text classification and LLMs functioning as classifiers.                                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Gradient-based attacks</span>:                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Methods that use the model gradients to guide perturbations in the text input to mislead the model outputs, such\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>as the work by Guo et al. (2021) [3].                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">White-box attacks at the token level (e.g., HotFlip)</span>:                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>White-box attacks that flip tokens based on gradient information to change the model's prediction with minimal  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>edits introduced to the text, exemplified by Ebrahimi et al. (2018) [4].                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">Universal adversarial triggers</span>:                                                                                 \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>These are input triggers—specific sequences of tokens—that, when prepended or appended to any input, can        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>consistently cause the model to produce incorrect or undesired outputs, as shown by Wallace et al. (2019) [5].  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 5 </span><span style=\"font-weight: bold\">Semantically equivalent adversarial rules</span>:                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Creating adversarial examples by applying transformations that preserve semantic meaning but fool the model,    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>following approaches like Ribeiro et al. (2018) [2].                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 6 </span><span style=\"font-weight: bold\">Imperceptible or subtle toxic triggers</span>:                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Tiny perturbations designed to inject toxic or harmful content into otherwise safe prompts, as studied by       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Mehrabi et al. (2022) [6].                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 7 </span><span style=\"font-weight: bold\">Attacks targeting text generation</span> (implied but less explicitly described):                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Attackers manipulate prompts or inputs to generate harmful, biased, or misleading outputs from LLMs.            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "<span style=\"font-weight: bold\">Summary:</span>                                                                                                           \n",
       "Recent adversarial attacks on LLMs include gradient-based token perturbations, white-box token flips (HotFlip),    \n",
       "universal adversarial triggers that work across inputs, semantically preserving adversarial substitutions, and     \n",
       "imperceptible toxicity injections. These strategies exploit LLM vulnerabilities both in classification and         \n",
       "generation tasks to cause undesired output behaviors.                                                              \n",
       "\n",
       "For full details, see the referenced article:                                                                      \n",
       "Weng, Lilian. (Oct 2023). “Adversarial Attacks on LLMs”. Lil’Log.                                                  \n",
       "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Based on Lilian Weng’s October 2023 article \u001b[3m“Adversarial Attacks on LLMs”\u001b[0m, recent types of adversarial attacks on  \n",
       "large language models (LLMs) can be broadly categorized as follows:                                                \n",
       "\n",
       "\u001b[1;33m 1 \u001b[0m\u001b[1mAdversarial attacks on classifiers\u001b[0m:                                                                             \n",
       "\u001b[1;33m   \u001b[0mThese attacks attempt to slightly perturb an input text (\\mathbf{x}) to produce an adversarial example          \n",
       "\u001b[1;33m   \u001b[0m(\\mathbf{x}\u001b[3m\\text{adv}) that causes the model’s classifier (f(.)) to change its prediction, i.e., (f(\\mathbf{x}) \u001b[0m\n",
       "\u001b[1;33m   \u001b[0m\u001b[3m\\neq f(\\mathbf{x}\u001b[0m\\text{adv})), while keeping the modifications imperceptible or semantically similar to humans. \n",
       "\u001b[1;33m   \u001b[0mThis category is inspired by classical adversarial attacks widely studied in image classification and adapted to\n",
       "\u001b[1;33m   \u001b[0mtext classification and LLMs functioning as classifiers.                                                        \n",
       "\u001b[1;33m 2 \u001b[0m\u001b[1mGradient-based attacks\u001b[0m:                                                                                         \n",
       "\u001b[1;33m   \u001b[0mMethods that use the model gradients to guide perturbations in the text input to mislead the model outputs, such\n",
       "\u001b[1;33m   \u001b[0mas the work by Guo et al. (2021) [3].                                                                           \n",
       "\u001b[1;33m 3 \u001b[0m\u001b[1mWhite-box attacks at the token level (e.g., HotFlip)\u001b[0m:                                                           \n",
       "\u001b[1;33m   \u001b[0mWhite-box attacks that flip tokens based on gradient information to change the model's prediction with minimal  \n",
       "\u001b[1;33m   \u001b[0medits introduced to the text, exemplified by Ebrahimi et al. (2018) [4].                                        \n",
       "\u001b[1;33m 4 \u001b[0m\u001b[1mUniversal adversarial triggers\u001b[0m:                                                                                 \n",
       "\u001b[1;33m   \u001b[0mThese are input triggers—specific sequences of tokens—that, when prepended or appended to any input, can        \n",
       "\u001b[1;33m   \u001b[0mconsistently cause the model to produce incorrect or undesired outputs, as shown by Wallace et al. (2019) [5].  \n",
       "\u001b[1;33m 5 \u001b[0m\u001b[1mSemantically equivalent adversarial rules\u001b[0m:                                                                      \n",
       "\u001b[1;33m   \u001b[0mCreating adversarial examples by applying transformations that preserve semantic meaning but fool the model,    \n",
       "\u001b[1;33m   \u001b[0mfollowing approaches like Ribeiro et al. (2018) [2].                                                            \n",
       "\u001b[1;33m 6 \u001b[0m\u001b[1mImperceptible or subtle toxic triggers\u001b[0m:                                                                         \n",
       "\u001b[1;33m   \u001b[0mTiny perturbations designed to inject toxic or harmful content into otherwise safe prompts, as studied by       \n",
       "\u001b[1;33m   \u001b[0mMehrabi et al. (2022) [6].                                                                                      \n",
       "\u001b[1;33m 7 \u001b[0m\u001b[1mAttacks targeting text generation\u001b[0m (implied but less explicitly described):                                      \n",
       "\u001b[1;33m   \u001b[0mAttackers manipulate prompts or inputs to generate harmful, biased, or misleading outputs from LLMs.            \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "\u001b[1mSummary:\u001b[0m                                                                                                           \n",
       "Recent adversarial attacks on LLMs include gradient-based token perturbations, white-box token flips (HotFlip),    \n",
       "universal adversarial triggers that work across inputs, semantically preserving adversarial substitutions, and     \n",
       "imperceptible toxicity injections. These strategies exploit LLM vulnerabilities both in classification and         \n",
       "generation tasks to cause undesired output behaviors.                                                              \n",
       "\n",
       "For full details, see the referenced article:                                                                      \n",
       "Weng, Lilian. (Oct 2023). “Adversarial Attacks on LLMs”. Lil’Log.                                                  \n",
       "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What are recent types of adversarial attacks in LLM?\"\n",
    "\n",
    "response = graph.invoke({\"question\": query})\n",
    "rprint(response)\n",
    "rprint(Markdown(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d74222a1-7085-4ecc-adc8-df307f7178e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'How does the AlphaCodium paper work?'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'retrieval_grade'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RetrievalGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The user is asking about how the AlphaCodium paper works, which likely requires detailed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information on the methodology, approach, and findings of the AlphaCodium paper. Without the content or summary of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the AlphaCodium paper provided in the current conversation, I do not have sufficient information to answer this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">question thoroughly. Therefore, retrieval of additional documents about the AlphaCodium paper is necessary.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">is_required</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'documents'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'99f01c59-8b0f-4f77-8c5c-ebbedd234e80'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Case Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">is provided with a list of tool names, descriptions of their utility, and details about the expected </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'6aeaf1d5-1c9e-4e06-828e-d381c6eb11e5'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Effectiveness is measured by attack objective functions designed for different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments:\\n- In text-to-image experiment, they used Q16 (Schramowski et al. 2022) and NudeNet </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(https://github.com/notAI-tech/NudeNet).\\n- text-to-text experiment: TOXIGEN\\nDiversity is measured by pairwise </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dissimilarity, in form of $\\\\sum_{(\\\\mathbf{x}_i, \\\\mathbf{x}_j) \\\\in \\\\text{All pairs}} [1 - </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\text{sim}(\\\\mathbf{x}_i, \\\\mathbf{x}_j)]$\\nLow-toxicity is measured by Perspective API.\\n\\n\\nScoring-LIFO: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Combine LIFO and Scoring strategies and force to update the last entry if the queue hasn’t been updated for a long </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">time.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'24a921a4-e750-4095-8a3c-57d295cece3e'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Fig. 9. Average attack success rate on \"HB (harmful behavior)\" instructions, averaging 5 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompts. Two baselines are \"HB\" prompt only or HB prompt followed by `\"Sure here\\'s\"` as a suffix. \"Concatenation\" </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combines several adversarial suffixes to construct a more powerful attack with a significantly higher success rate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in some cases. \"Ensemble\" tracks if any of 5 prompts and the concatenated one succeeded. (Image source: Zou et al. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2023)\\nARCA (“Autoregressive Randomized Coordinate Ascent”; Jones et al. 2023) considers a broader set of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">optimization problems to find input-output pairs $(\\\\mathbf{x}, \\\\mathbf{y})$ that match certain behavior pattern; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such as non-toxic input starting with \"Barack Obama\" but leading to toxic output. Given an auditing objective </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">$\\\\phi: \\\\mathcal{X} \\\\times \\\\mathcal{Y} \\\\to \\\\mathbb{R}$ that maps a pair of (input prompt, output completion) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">into scores. Examples of behavior patterns captured by $\\\\phi$ are as follows:'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'c5b6d596-85f8-4a50-8842-3786b0f424b3'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://lilianweng.github.io/posts/2023-06-23-agent/'</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Fig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">than inefficient planning in AlfWorld. (Image source: Shinn &amp; Labash, 2023)'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'relevance_grades'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelevanceGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The user question is specifically about how the AlphaCodium paper works. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieved document discusses ChemCrow, an example of using large language models augmented with expert tools for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks in organic synthesis, drug discovery, and materials design. It mentions the workflow, tool integration, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning approach but does not mention AlphaCodium or related topics. Therefore, the document does not contain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information related to the AlphaCodium paper or how it works.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelevanceGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The user question asks specifically about how the AlphaCodium paper works. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieved document discusses evaluation metrics for attacks, such as effectiveness in text-to-image and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">text-to-text experiments, diversity measures, and low-toxicity metrics. However, the content does not mention </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">AlphaCodium or provide any details about it. Therefore, the document does not address the question of how </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">AlphaCodium works and is not relevant.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelevanceGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The user question is about the workings of the AlphaCodium paper. The retrieved </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document discusses attack success rates on harmful behavior instructions, adversarial suffixes, and ARCA </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">optimization methods in finding input-output pairs with specific behavior patterns. There is no mention or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">indication of the AlphaCodium paper or its methods in the retrieved text. Therefore, the document does not provide </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">relevant information about how the AlphaCodium paper works.'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelevanceGrade</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">chain_of_thought</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The user question is about the 'AlphaCodium' paper and how it works. The retrieved </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">document talks about experiments on AlfWorld Env and HotpotQA, mentioning issues like hallucination and inefficient</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">planning, and cites Shinn &amp; Labash, 2023. There is no mention or apparent connection to 'AlphaCodium' or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">description of that paper's workings. Hence, the document does not address the user's question.\"</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">is_relevant</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"I don't have an answer to the question.\"</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'question'\u001b[0m: \u001b[32m'How does the AlphaCodium paper work?'\u001b[0m,\n",
       "    \u001b[32m'retrieval_grade'\u001b[0m: \u001b[1;35mRetrievalGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The user is asking about how the AlphaCodium paper works, which likely requires detailed \u001b[0m\n",
       "\u001b[32minformation on the methodology, approach, and findings of the AlphaCodium paper. Without the content or summary of \u001b[0m\n",
       "\u001b[32mthe AlphaCodium paper provided in the current conversation, I do not have sufficient information to answer this \u001b[0m\n",
       "\u001b[32mquestion thoroughly. Therefore, retrieval of additional documents about the AlphaCodium paper is necessary.'\u001b[0m,\n",
       "        \u001b[33mis_required\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'documents'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'99f01c59-8b0f-4f77-8c5c-ebbedd234e80'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Case Studies#\\nScientific Discovery Agent#\\nChemCrow \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBran et al. 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is a \u001b[0m\n",
       "\u001b[32mdomain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic \u001b[0m\n",
       "\u001b[32msynthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was \u001b[0m\n",
       "\u001b[32mpreviously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM\u001b[0m\n",
       "\u001b[32mis provided with a list of tool names, descriptions of their utility, and details about the expected \u001b[0m\n",
       "\u001b[32minput/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The \u001b[0m\n",
       "\u001b[32minstruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'6aeaf1d5-1c9e-4e06-828e-d381c6eb11e5'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Effectiveness is measured by attack objective functions designed for different \u001b[0m\n",
       "\u001b[32mexperiments:\\n- In text-to-image experiment, they used Q16 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSchramowski et al. 2022\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and NudeNet \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mhttps://github.com/notAI-tech/NudeNet\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n- text-to-text experiment: TOXIGEN\\nDiversity is measured by pairwise \u001b[0m\n",
       "\u001b[32mdissimilarity, in form of $\\\\sum_\u001b[0m\u001b[32m{\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_i, \\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_j\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\\\in \\\\text\u001b[0m\u001b[32m{\u001b[0m\u001b[32mAll pairs\u001b[0m\u001b[32m}\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m1 - \u001b[0m\n",
       "\u001b[32m\\\\text\u001b[0m\u001b[32m{\u001b[0m\u001b[32msim\u001b[0m\u001b[32m}\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_i, \\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_j\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m$\\nLow-toxicity is measured by Perspective API.\\n\\n\\nScoring-LIFO: \u001b[0m\n",
       "\u001b[32mCombine LIFO and Scoring strategies and force to update the last entry if the queue hasn’t been updated for a long \u001b[0m\n",
       "\u001b[32mtime.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'24a921a4-e750-4095-8a3c-57d295cece3e'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Fig. 9. Average attack success rate on \"HB \u001b[0m\u001b[32m(\u001b[0m\u001b[32mharmful behavior\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\" instructions, averaging 5 \u001b[0m\n",
       "\u001b[32mprompts. Two baselines are \"HB\" prompt only or HB prompt followed by `\"Sure here\\'s\"` as a suffix. \"Concatenation\" \u001b[0m\n",
       "\u001b[32mcombines several adversarial suffixes to construct a more powerful attack with a significantly higher success rate \u001b[0m\n",
       "\u001b[32min some cases. \"Ensemble\" tracks if any of 5 prompts and the concatenated one succeeded. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Zou et al. \u001b[0m\n",
       "\u001b[32m2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nARCA \u001b[0m\u001b[32m(\u001b[0m\u001b[32m“Autoregressive Randomized Coordinate Ascent”; Jones et al. 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m considers a broader set of \u001b[0m\n",
       "\u001b[32moptimization problems to find input-output pairs $\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \\\\mathbf\u001b[0m\u001b[32m{\u001b[0m\u001b[32my\u001b[0m\u001b[32m}\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$ that match certain behavior pattern; \u001b[0m\n",
       "\u001b[32msuch as non-toxic input starting with \"Barack Obama\" but leading to toxic output. Given an auditing objective \u001b[0m\n",
       "\u001b[32m$\\\\phi: \\\\mathcal\u001b[0m\u001b[32m{\u001b[0m\u001b[32mX\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \\\\times \\\\mathcal\u001b[0m\u001b[32m{\u001b[0m\u001b[32mY\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \\\\to \\\\mathbb\u001b[0m\u001b[32m{\u001b[0m\u001b[32mR\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$ that maps a pair of \u001b[0m\u001b[32m(\u001b[0m\u001b[32minput prompt, output completion\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32minto scores. Examples of behavior patterns captured by $\\\\phi$ are as follows:'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mid\u001b[0m=\u001b[32m'c5b6d596-85f8-4a50-8842-3786b0f424b3'\u001b[0m,\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'https://lilianweng.github.io/posts/2023-06-23-agent/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Fig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure \u001b[0m\n",
       "\u001b[32mthan inefficient planning in AlfWorld. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Shinn & Labash, 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'relevance_grades'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mRelevanceGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The user question is specifically about how the AlphaCodium paper works. The \u001b[0m\n",
       "\u001b[32mretrieved document discusses ChemCrow, an example of using large language models augmented with expert tools for \u001b[0m\n",
       "\u001b[32mtasks in organic synthesis, drug discovery, and materials design. It mentions the workflow, tool integration, and \u001b[0m\n",
       "\u001b[32mreasoning approach but does not mention AlphaCodium or related topics. Therefore, the document does not contain \u001b[0m\n",
       "\u001b[32minformation related to the AlphaCodium paper or how it works.'\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mRelevanceGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The user question asks specifically about how the AlphaCodium paper works. The \u001b[0m\n",
       "\u001b[32mretrieved document discusses evaluation metrics for attacks, such as effectiveness in text-to-image and \u001b[0m\n",
       "\u001b[32mtext-to-text experiments, diversity measures, and low-toxicity metrics. However, the content does not mention \u001b[0m\n",
       "\u001b[32mAlphaCodium or provide any details about it. Therefore, the document does not address the question of how \u001b[0m\n",
       "\u001b[32mAlphaCodium works and is not relevant.'\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mRelevanceGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m'The user question is about the workings of the AlphaCodium paper. The retrieved \u001b[0m\n",
       "\u001b[32mdocument discusses attack success rates on harmful behavior instructions, adversarial suffixes, and ARCA \u001b[0m\n",
       "\u001b[32moptimization methods in finding input-output pairs with specific behavior patterns. There is no mention or \u001b[0m\n",
       "\u001b[32mindication of the AlphaCodium paper or its methods in the retrieved text. Therefore, the document does not provide \u001b[0m\n",
       "\u001b[32mrelevant information about how the AlphaCodium paper works.'\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mRelevanceGrade\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mchain_of_thought\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m user question is about the 'AlphaCodium' paper and how it works. The retrieved \u001b[0m\n",
       "\u001b[32mdocument talks about experiments on AlfWorld Env and HotpotQA, mentioning issues like hallucination and inefficient\u001b[0m\n",
       "\u001b[32mplanning, and cites Shinn & Labash, 2023. There is no mention or apparent connection to 'AlphaCodium' or \u001b[0m\n",
       "\u001b[32mdescription of that paper's workings. Hence, the document does not address the user's question.\"\u001b[0m,\n",
       "            \u001b[33mis_relevant\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'context'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'answer'\u001b[0m: \u001b[32m\"I don't have an answer to the question.\"\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I don't have an answer to the question.                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "I don't have an answer to the question.                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"How does the AlphaCodium paper work?\"\n",
    "\n",
    "response = graph.invoke({\"question\": query})\n",
    "rprint(response)\n",
    "rprint(Markdown(response[\"answer\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
